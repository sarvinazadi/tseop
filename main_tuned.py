#main_tuned.py
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import torch.nn as nn
import os

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ
from tse_env import TSEPortfolioEnv
from data_loader import fetch_and_clean_data

# ==========================================
# âš™ï¸ TUNED HYPERPARAMETERS (From Optuna)
# ==========================================
# Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ù‚ÛŒÙ‚ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ø´Ù…Ø§
TUNED_PARAMS = {
    'learning_rate': 0.00010010429449913118,
    'gamma': 0.9850740145072168,
    'gae_lambda': 0.9409478201585882,
    'ent_coef': 1.819821976503584e-05,
    'batch_size': 64,
    'n_steps': 1024,  # Ø¨Ø§ÛŒØ¯ Ø¶Ø±ÛŒØ¨ÛŒ Ø§Ø² batch_size Ø¨Ø§Ø´Ø¯
}

# Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡ (Large)
POLICY_KWARGS = dict(
    activation_fn=nn.Tanh,
    net_arch=[dict(pi=[256, 256], vf=[256, 256])]  # Large Architecture
)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒ
TARGET_TICKERS = ["ÙÙˆÙ„Ø§Ø¯", "Ø´Ù¾Ù†Ø§", "ÙˆØ¨Ù…Ù„Øª", "ÙÙ…Ù„ÛŒ", "Ø´Ø³ØªØ§", "Ø®ÙˆØ¯Ø±Ùˆ", "ÙˆØªØ¬Ø§Ø±Øª"]
TOTAL_TIMESTEPS = 200000  # Ø¢Ù…ÙˆØ²Ø´ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ú©Ø§Ù…Ù„
MODEL_NAME = "TSE_Genius_Agent_v6_TUNED"

def train_tuned_agent():
    print(f"ğŸš€ Starting Final Training for {MODEL_NAME}...")
    
    # 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    print("â³ Loading Data...")
    data, dates, tickers = fetch_and_clean_data(TARGET_TICKERS)
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§ Ù‚Ø¨Ù„ Ø§Ø² 2023 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
    # ÛŒØ§ Ø­ØªÛŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙØ±Ø¯Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….
    # Ø§Ù…Ø§ Ø·Ø¨Ù‚ Ø±ÙˆØ§Ù„ Ø¹Ù„Ù…ÛŒØŒ ØªØ§ 2023 Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ø¨Ø§ ØªØ³Øª Ù‚Ø¨Ù„ÛŒ Ù‚Ø§Ø¨Ù„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§Ø´Ø¯)
    SPLIT_DATE = "2023-01-01"
    split_idx = -1
    for i, date_str in enumerate(dates):
        if date_str >= SPLIT_DATE:
            split_idx = i
            break
            
    if split_idx == -1: split_idx = int(len(dates) * 0.8)

    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ (ØªØ§ 2023)
    train_data = data[:split_idx]
    train_dates = dates[:split_idx]
    
    print(f"ğŸ“… Training Data: {len(train_dates)} days (End: {train_dates[-1]})")

    # 2. Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ·
    env = DummyVecEnv([lambda: TSEPortfolioEnv(
        data=train_data,
        dates=train_dates,
        tickers=tickers,
        initial_amount=1e8,
        transaction_cost_pct=0.0015,
        window_size=30
    )])

    # 3. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªÛŒÙˆÙ† Ø´Ø¯Ù‡
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=TUNED_PARAMS['learning_rate'],
        n_steps=TUNED_PARAMS['n_steps'],
        batch_size=TUNED_PARAMS['batch_size'],
        gamma=TUNED_PARAMS['gamma'],
        gae_lambda=TUNED_PARAMS['gae_lambda'],
        ent_coef=TUNED_PARAMS['ent_coef'],
        policy_kwargs=POLICY_KWARGS,
        verbose=1,
        tensorboard_log="./ppo_tse_tuned_tensorboard/"
    )

    # 4. Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´
    print(f"ğŸ‹ï¸â€â™‚ï¸ Training for {TOTAL_TIMESTEPS} timesteps...")
    model.learn(total_timesteps=TOTAL_TIMESTEPS)

    # 5. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
    model.save(MODEL_NAME)
    print(f"âœ… Model saved as '{MODEL_NAME}.zip'")

if __name__ == "__main__":
    train_tuned_agent()
