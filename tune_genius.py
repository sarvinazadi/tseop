import optuna
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.evaluation import evaluate_policy
import torch.nn as nn
import os

# --- Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ Ø´Ù…Ø§ ---
from tse_env import TSEPortfolioEnv
from data_loader import fetch_and_clean_data

# Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ (Ù‡Ù…Ø§Ù† Ù„ÛŒØ³Øª Ø§ØµÙ„ÛŒ)
TARGET_TICKERS = ["ÙÙˆÙ„Ø§Ø¯", "Ø´Ù¾Ù†Ø§", "ÙˆØ¨Ù…Ù„Øª", "ÙÙ…Ù„ÛŒ", "Ø´Ø³ØªØ§", "Ø®ÙˆØ¯Ø±Ùˆ", "ÙˆØªØ¬Ø§Ø±Øª"]
SPLIT_DATE = "2023-01-01"

def load_and_split_data():
    """
    Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    print("â³ Loading data from PKL...")
    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø¯ÛŒØªØ§Ù„ÙˆØ¯Ø± Ø®ÙˆØ¯ØªØ§Ù†
    data, dates, tickers = fetch_and_clean_data(TARGET_TICKERS, force_update=False)
    
    if data is None:
        raise ValueError("âŒ Data load failed! Please check clean_market_data.pkl")

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø´ Ø²Ù…Ø§Ù†ÛŒ (2023-01-01)
    split_idx = -1
    for i, date_str in enumerate(dates):
        if date_str >= SPLIT_DATE:
            split_idx = i
            break
    
    if split_idx == -1:
        print("âš ï¸ Warning: Split date not found. Using 80% split.")
        split_idx = int(len(dates) * 0.8)

    print(f"âœ‚ï¸ Splitting data at index {split_idx} ({dates[split_idx]})")

    # Ø¨Ø±Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Numpy
    # data shape: (Days, Assets, Features)
    train_data = data[:split_idx]
    train_dates = dates[:split_idx]
    
    val_data = data[split_idx:]
    val_dates = dates[split_idx:]

    return (train_data, train_dates), (val_data, val_dates), tickers

# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª) ---
(train_d, train_dates), (val_d, val_dates), valid_tickers = load_and_split_data()

def objective(trial):
    """
    ØªØ§Ø¨Ø¹ Ù‡Ø¯Ù Optuna: Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø±Ø§ ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¨Ø§Ø²Ø¯Ù‡ÛŒ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    
    # 1. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (Hyperparameters)
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-4, log=True)
    gamma = trial.suggest_float("gamma", 0.9, 0.9999)
    gae_lambda = trial.suggest_float("gae_lambda", 0.9, 1.0)
    ent_coef = trial.suggest_float("ent_coef", 1e-8, 0.01, log=True)
    batch_size = trial.suggest_categorical("batch_size", [64, 128, 256])
    n_steps = trial.suggest_categorical("n_steps", [1024, 2048])
    
    # Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡ (ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÙ‡ Ùˆ Ù†ÙˆØ±ÙˆÙ†)
    net_arch_type = trial.suggest_categorical("net_arch", ["medium", "large"])
    if net_arch_type == "medium":
        net_arch = [dict(pi=[128, 128], vf=[128, 128])]
    else: # large
        net_arch = [dict(pi=[256, 256], vf=[256, 256])]

    policy_kwargs = dict(
        activation_fn=nn.Tanh, # Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù„ÛŒ Tanh Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ù‡ØªØ± Ø§Ø³Øª
        net_arch=net_arch
    )

    # 2. Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ· Ø¢Ù…ÙˆØ²Ø´
    train_env = DummyVecEnv([lambda: TSEPortfolioEnv(
        data=train_d,
        dates=train_dates,
        tickers=valid_tickers,
        initial_amount=1e8,
        transaction_cost_pct=0.0015, # Ú©Ø§Ø±Ù…Ø²Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ
        window_size=30
    )])

    # 3. Ø³Ø§Ø®Øª Ù…Ø­ÛŒØ· Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (Validation)
    # Ù…Ø¯Ù„ Ø±Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ (Ø¨Ø¹Ø¯ Ø§Ø² 2023) ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒÙ… ÙˆØ§Ù‚Ø¹Ø§ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡
    val_env = DummyVecEnv([lambda: TSEPortfolioEnv(
        data=val_d,
        dates=val_dates,
        tickers=valid_tickers,
        initial_amount=1e8,
        transaction_cost_pct=0.0015,
        window_size=30
    )])

    # 4. ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„
    model = PPO(
        "MlpPolicy",
        train_env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        gamma=gamma,
        gae_lambda=gae_lambda,
        ent_coef=ent_coef,
        policy_kwargs=policy_kwargs,
        verbose=0
    )

    # 5. Ø¢Ù…ÙˆØ²Ø´ Ø³Ø±ÛŒØ¹
    # ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³ØªÙ¾ Ú©Ù… (Ù…Ø«Ù„Ø§ 30,000) ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ù†Ù‡
    # Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ø§ØµÙ„ÛŒ (Main) Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø±Ø§ 200,000 Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ…
    try:
        model.learn(total_timesteps=30000)
    except Exception as e:
        print(f"âŒ Error in trial: {e}")
        return -1e9 # Ø¬Ø±ÛŒÙ…Ù‡ Ø³Ù†Ú¯ÛŒÙ†

    # 6. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø± Ù…Ø­ÛŒØ· Validation (Ø¢ÛŒÙ†Ø¯Ù‡)
    # Ø§ÛŒÙ† Ù…Ù‡Ù… Ø§Ø³Øª: Ù…Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ú©Ù‡ Ø¯Ø± "Ø¢ÛŒÙ†Ø¯Ù‡" Ø³ÙˆØ¯ Ø¨Ø¯Ù‡Ø¯
    mean_reward, _ = evaluate_policy(model, val_env, n_eval_episodes=1)
    
    return mean_reward

if __name__ == "__main__":
    print("\nğŸš€ Starting Hyperparameter Tuning for TSE Genius V6...")
    print(f"   Target Tickers: {len(valid_tickers)}")
    print(f"   Training Data Days: {len(train_dates)}")
    print(f"   Validation Data Days: {len(val_dates)}")
    
    # Ø³Ø§Ø®Øª Ù…Ø·Ø§Ù„Ø¹Ù‡ (Study)
    study = optuna.create_study(direction="maximize")
    
    # ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡Ø§ÛŒ ØªÙ„Ø§Ø´ (Ù‡Ø± Ú†Ù‚Ø¯Ø± Ø¨ÛŒØ´ØªØ±ØŒ Ù†ØªÛŒØ¬Ù‡ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ù…Ø§ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±)
    # ÙØ¹Ù„Ø§Ù‹ Ø±ÙˆÛŒ Û²Û° Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ… Ú©Ù‡ Ø­Ø¯ÙˆØ¯ Û±Ûµ-Û²Û° Ø¯Ù‚ÛŒÙ‚Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯
    study.optimize(objective, n_trials=20)
    
    print("\nâœ… Tuning Finished!")
    print("ğŸ† Best Value (Reward):", study.best_value)
    print("ğŸ”§ Best Hyperparameters:")
    for key, value in study.best_params.items():
        print(f"    {key}: {value}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ
    with open("best_hyperparameters_v6.txt", "w") as f:
        f.write(str(study.best_params))
    
    print("\nğŸ’¾ Best params saved to 'best_hyperparameters_v6.txt'")
