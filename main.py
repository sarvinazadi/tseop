import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
import os
import torch.nn as nn

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ù…Ø§Ù†
from tse_env import TSEPortfolioEnv
from data_loader import fetch_and_clean_data

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙØ§Ø±Ø³ÛŒ/Ø²ÛŒØ¨Ø§
plt.rcParams['figure.figsize'] = (12, 6)
plt.style.use('ggplot')

TARGET_TICKERS = ["ÙÙˆÙ„Ø§Ø¯", "Ø´Ù¾Ù†Ø§", "ÙˆØ¨Ù…Ù„Øª", "ÙÙ…Ù„ÛŒ", "Ø´Ø³ØªØ§", "Ø®ÙˆØ¯Ø±Ùˆ", "ÙˆØªØ¬Ø§Ø±Øª"]

def run_backtest_and_plot(env, model, valid_tickers):
    """
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù…Ø³Ø¦ÙˆÙ„ Ø§Ø¬Ø±Ø§ÛŒ RolloutØŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ú©Ø³Ù„ Ùˆ Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø³Øª.
    """
    print("\nğŸ“‰ Starting Backtest (Rollout)...")
    
    obs, info = env.reset()
    done = False
    
    # Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡
    history = []
    
    while not done:
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ú©Ø´Ù† (Ø¨Ø¯ÙˆÙ† Ø­Ø§Ù„Øª ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª)
        action, _ = model.predict(obs, deterministic=True)
        
        obs, reward, done, truncated, info = env.step(action)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÛŒÙ† Ø±ÙˆØ²
        day_record = {
            'Date': info['date'],
            'Portfolio Value': info['portfolio_value'],
            'Agent Return (%)': info['portfolio_return'],
            'Market Return (%)': info['market_return'],
            'Cash': info['cash_balance'],
            'Reward': reward
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ²Ù† Ù‡Ø± Ø³Ù‡Ù… Ø¨Ù‡ Ø±Ú©ÙˆØ±Ø¯ (Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø³Ù„)
        allocations = info['allocations']
        for idx, ticker in enumerate(valid_tickers):
            # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡Ù…â€ŒÙ‡Ø§ Ú©Ù…ØªØ± Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ ØªÛŒÚ©Ø±Ù‡Ø§ Ø¨ÙˆØ¯ (Ø¨Ù‡ Ù‡Ø± Ø¯Ù„ÛŒÙ„ÛŒ)ØŒ Ù‡Ù†Ø¯Ù„ Ú©Ù†
            if idx < len(allocations):
                day_record[f"Alloc_{ticker}"] = allocations[idx]
        
        history.append(day_record)

        # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ù‡Ø± 200 Ø±ÙˆØ²
        if len(history) % 200 == 0:
            print(f"   ğŸ“… {info['date']} | Value: {info['portfolio_value']:,.0f} | Return: {info['portfolio_return']:.2f}%")

    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
    df_res = pd.DataFrame(history)
    df_res['Date'] = pd.to_datetime(df_res['Date'])
    df_res.set_index('Date', inplace=True)
    
    # -------------------------------------------------------
    # 1. Ø®Ø±ÙˆØ¬ÛŒ Ø§Ú©Ø³Ù„
    # -------------------------------------------------------
    excel_path = "backtest_results.xlsx"
    df_res.to_excel(excel_path)
    print(f"\nâœ… Excel report saved to: {excel_path}")

    # -------------------------------------------------------
    # 2. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ (Ø³ÙˆØ¯ Ø§ÛŒØ¬Ù†Øª vs Ø¨Ø§Ø²Ø§Ø±)
    # -------------------------------------------------------
    plt.figure(figsize=(12, 6))
    plt.plot(df_res.index, df_res['Agent Return (%)'], label='AI Agent', color='blue', linewidth=2)
    plt.plot(df_res.index, df_res['Market Return (%)'], label='Market (Benchmark)', color='gray', linestyle='--', alpha=0.7)
    
    plt.title("AI Agent vs Market Performance")
    plt.ylabel("Cumulative Return (%)")
    plt.xlabel("Date")
    plt.legend()
    plt.grid(True)
    
    perf_path = "chart_performance.png"
    plt.savefig(perf_path)
    print(f"ğŸ“Š Performance chart saved to: {perf_path}")
    plt.show() # Ù†Ù…Ø§ÛŒØ´ Ù¾Ù†Ø¬Ø±Ù‡ (Ø§Ú¯Ø± Ø±ÙˆÛŒ Ø³ÛŒØ³ØªÙ… Ù„ÙˆÚ©Ø§Ù„ Ù‡Ø³ØªÛŒØ¯)

    # -------------------------------------------------------
    # 3. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ®ØµÛŒØµ Ø¯Ø§Ø±Ø§ÛŒÛŒ (Asset Allocation Area Chart)
    # -------------------------------------------------------
    # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ØªØ®ØµÛŒØµ
    alloc_cols = [c for c in df_res.columns if c.startswith("Alloc_")]
    
    if alloc_cols:
        plt.figure(figsize=(12, 6))
        plt.stackplot(df_res.index, df_res[alloc_cols].T, labels=[c.replace("Alloc_", "") for c in alloc_cols], alpha=0.8)
        plt.title("Portfolio Asset Allocation Over Time")
        plt.ylabel("Allocation Ratio (0.0 to 1.0)")
        plt.xlabel("Date")
        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
        plt.tight_layout()
        
        alloc_path = "chart_allocation.png"
        plt.savefig(alloc_path)
        print(f"ğŸ¨ Allocation chart saved to: {alloc_path}")
        plt.show()

    # Ú†Ø§Ù¾ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
    final_val = df_res['Portfolio Value'].iloc[-1]
    final_ret = df_res['Agent Return (%)'].iloc[-1]
    print(f"\nğŸ FINAL RESULTS:")
    print(f"   Initial Wealth: 100,000,000")
    print(f"   Final Wealth:   {final_val:,.0f}")
    print(f"   Total Return:   {final_ret:.2f}%")


def main():
    # 1. Ù„ÙˆØ¯ Ø¯ÛŒØªØ§
    print("ğŸš€ Loading data...")
    data, dates, valid_tickers = fetch_and_clean_data(TARGET_TICKERS, force_update=False)
    
    if data is None:
        return

    # 2. Ù…Ø­ÛŒØ·
    env = TSEPortfolioEnv(
        data=data,
        dates=dates,
        tickers=valid_tickers,
        initial_amount=1e8,
        transaction_cost_pct=0.0015,
        window_size=30
    )

# ØªØ¹Ø±ÛŒÙ Ø³Ø§Ø®ØªØ§Ø± Ù…ØºØ² Ø¬Ø¯ÛŒØ¯ (Ú©Ù…ÛŒ Ø¨Ø²Ø±Ú¯ØªØ± Ùˆ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±)
# net_arch=[dict(pi=[128, 128], vf=[128, 128])] ÛŒØ¹Ù†ÛŒ:
# Ø¯Ùˆ Ù„Ø§ÛŒÙ‡ Û±Û²Û¸ ØªØ§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ (Policy)
# Ø¯Ùˆ Ù„Ø§ÛŒÙ‡ Û±Û²Û¸ ØªØ§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØ®Ù…ÛŒÙ† Ø§Ø±Ø²Ø´ (Value Function)
# Ø§Ú©ØªÛŒÙˆÛŒØ´Ù† Tanh Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ù‡ØªØ± Ø§Ø² ReLU Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    policy_kwargs = dict(
        activation_fn=nn.Tanh,
        net_arch=dict(pi=[128, 128], vf=[128, 128])
    )

    # 3. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
    TOTAL_TIMESTEPS = 200_000 
    print(f"\nğŸ§  Training PPO Agent ({TOTAL_TIMESTEPS} steps)...")
    
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        seed=42,

        learning_rate=2e-4,
        n_steps=2048,
        gamma=0.99,
        batch_size=64,
        ent_coef=0.005,
        policy_kwargs=policy_kwargs,
        tensorboard_log="./ppo_tse_logs/"
    )
    
    try:
        model.learn(total_timesteps=TOTAL_TIMESTEPS)
        print("âœ… Training Finished.")
        model.save("ppo_tse_agent_final")
    except Exception as e:
        print(f"âŒ Training Failed: {e}")
        return

    # 4. Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ú©â€ŒØªØ³Øª Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ (Ø¬Ø§Ù…Ù¾ Ø¨Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø± Ùˆ Ø§Ú©Ø³Ù„)
    # Ù…Ø­ÛŒØ· Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø±ÛŒØ³Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ø­ÛŒØ· Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡â€ŒØ§ÛŒ Ù‡Ù… Ø¨Ø³Ø§Ø²ÛŒØ¯)
    env.diagnosis_mode = False # Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø±Ø§ Ø®Ø§Ù…ÙˆØ´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø³Ø±Ø¹Øª Ø²ÛŒØ§Ø¯ Ø´ÙˆØ¯
    run_backtest_and_plot(env, model, valid_tickers)

# ==========================================
# Ø¨Ø®Ø´ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ (The Save Protocol)
# ==========================================

# 1. Ø°Ø®ÛŒØ±Ù‡ Ø®ÙˆØ¯ Ù…Ø¯Ù„ (Ù…ØºØ² Ø§ÛŒØ¬Ù†Øª)
    model_name = "TSE_Genius_Agent_v5"
    model.save(model_name)
    print(f"âœ… Model saved successfully as: {model_name}.zip")

    # 2. Ø°Ø®ÛŒØ±Ù‡ Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ø§Ú¯Ø± Ø§Ø² VecNormalize Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´ÛŒØ¯ Ù…Ù‡Ù… Ø§Ø³Øª)
    # Ø§Ú¯Ø± Ø§Ø² VecNormalize Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯ÛŒØ¯ØŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø±ÙˆØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù…Ø´Ú©Ù„ÛŒ Ù†ÛŒØ³Øª.
    try:
        env.save("vec_normalize.pkl")
        print("âœ… Environment normalization stats saved.")
    except:
        pass

    print("--- ÙØ±Ø§ÛŒÙ†Ø¯ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯ ---")

if __name__ == "__main__":
    main()
